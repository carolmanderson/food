{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZjQuQUjGmm_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import subprocess\n",
    "sys.path.append(\"../..\")\n",
    "from src.training.dataset_utils import read_conll_file, examples_to_indices\n",
    "from src.training.train_utils import evaluate_ner, form_ner_pred_matrix\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-vtVLRcGmnC"
   },
   "outputs": [],
   "source": [
    "base_path = \"/Users/Carol/Google Drive/\"\n",
    "dev_file = os.path.join(base_path, \"nlp_data/recipe_data/20200523_food_gold_dev.conll\")\n",
    "model_file = \"/Users/Carol/Google Drive/nlp_data/output/20200523_22_06_28/20200523_22_06_28_food_ner_epoch_5_dev_f1_0.9851520816163143.h5\"\n",
    "mappings_file = \"/Users/Carol/Google Drive/nlp_data/output/20200523_22_06_28/20200523_22_06_28_food_ner_mappings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNxrynaPGmnD"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6rW5SvHGmnD"
   },
   "outputs": [],
   "source": [
    "mappings = pickle.load(open(mappings_file, \"rb\"))\n",
    "label_to_index = mappings['label_to_index']\n",
    "token_to_index = mappings['token_to_index']\n",
    "index_to_label = {v:k for k,v in label_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEZKYZdYGmnD"
   },
   "outputs": [],
   "source": [
    "dev_dataset = read_conll_file(dev_file)\n",
    "dev_sentences = examples_to_indices(dev_dataset, label_to_index, token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5IUDhdnGmnD"
   },
   "outputs": [],
   "source": [
    "label_mappings = list(index_to_label.items())\n",
    "label_mappings.sort()\n",
    "label_strings = [x[1] for x in label_mappings]\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for sent in dev_sentences:\n",
    "    preds = model.predict_on_batch(form_ner_pred_matrix(sent['tokens']))\n",
    "    y_pred.extend(np.argmax(preds, axis=-1)[0])\n",
    "    y_true.extend(sent['labels'])\n",
    "metrics = classification_report(y_true, y_pred, target_names = label_strings,\n",
    "                                output_dict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhzV_7b5GmnE",
    "outputId": "8473ec09-53d7-458f-9da8-a24749a562a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      I-FOOD       0.93      0.86      0.89       278\n",
      "           O       0.99      0.99      0.99      9464\n",
      "      B-FOOD       0.95      0.94      0.95      1175\n",
      "\n",
      "    accuracy                           0.99     10917\n",
      "   macro avg       0.96      0.93      0.94     10917\n",
      "weighted avg       0.99      0.99      0.99     10917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# token-level metrics\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH6sD-ocGmnF"
   },
   "outputs": [],
   "source": [
    "# write to conll format for use of conll perl script to calculate entity-level metrics\n",
    "outfile = \"/Users/Carol/Google Drive/nlp_data/output/20200523_22_06_28/dev_conll.txt\"\n",
    "with open(outfile, \"w\") as out:\n",
    "    ctr = 0\n",
    "    for doc in dev_sentences:\n",
    "        for token in doc['raw_tokens']:\n",
    "            out.write(f\"{token} {index_to_label[y_true[ctr]]} {index_to_label[y_pred[ctr]]}\\n\")\n",
    "            ctr += 1\n",
    "        out.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZ19duGMGmnF",
    "outputId": "ddac39c4-d85b-44e0-9b6d-e51fd49323ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = \"/Users/Carol/Google Drive/nlp_data/output/20200523_22_06_28/dev_conll.txt\"\n",
    "outfile = '/Users/Carol/Google Drive/nlp_data/output/20200523_22_06_28/eval.txt'\n",
    "conlleval_script_path = \"/Users/Carol/Dropbox/repos/food/src/evaluation\"\n",
    "os.chdir(conlleval_script_path)\n",
    "cmd = \"perl conlleval.pl < {} > {}\".format(json.dumps(infile), json.dumps(outfile))\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soDoQ2IIGmnF"
   },
   "source": [
    "# Update June 2023 for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52989,
     "status": "ok",
     "timestamp": 1688613669030,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "QLPuL8vBGmnG",
    "outputId": "adae2000-dd9d-4ba7-f611-3bb233cb08ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers[torch])\n",
      "  Downloading huggingface_hub-0.16.2-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
      "Collecting accelerate>=0.20.2 (from transformers[torch])\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.6.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, accelerate\n",
      "Successfully installed accelerate-0.20.3 huggingface-hub-0.16.2 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2950 sha256=61e1b3a02dd13cb8b07732e12a958a4f19cd46e8b91757e37b4f44178958c66d\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/1f/8d/4f812c590e074c1e928f5cec67bf5053b71f38e2648739403a\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post5\n"
     ]
    }
   ],
   "source": [
    "LOCAL = False   # training on local Mac vs. in Colab\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "if LOCAL:\n",
    "    BASE_PATH = \"/Users/carolanderson/Dropbox/\"\n",
    "\n",
    "else:\n",
    "    BASE_PATH = \"/content/drive/My Drive/\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    ! pip install transformers[torch]\n",
    "    ! pip install sklearn\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1688616173572,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "ipHpcl9RGmnH"
   },
   "outputs": [],
   "source": [
    "def read_conll_file(file):\n",
    "    \"\"\"\n",
    "    Given a file in CoNLL format, read in tokens and labels. Treat each sentence as a training example.\n",
    "    :param file: file in CoNLL format; tokens are assumed to be in the first column and labels in the last column.\n",
    "    :returns a nested list, in which each sublist is a sentence and contains a sublist [token, label] for each token.\n",
    "\n",
    "    .. note:: Ignores document boundaries and treats each sentence as an independent training example.\n",
    "    \"\"\"\n",
    "    documents = []  # holds all documents\n",
    "    sentence = [] # will hold the first sentence\n",
    "    with open(file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            if '-DOCSTART-' in line:  # beginning of a new document; ignore this since we will treat each sentence as a training example\n",
    "                continue\n",
    "            elif not line.split():  # beginning of a new sentence\n",
    "                if sentence:\n",
    "                    documents.append(sentence)\n",
    "                sentence = []\n",
    "            else:\n",
    "                token, *other_columns, label = line.split()\n",
    "                sentence.append([token, label])\n",
    "    return documents\n",
    "\n",
    "\n",
    "def detokenize_conll_input(dataset):\n",
    "    '''\n",
    "    For each sentence in the conll file, rejoin the tokens with spaces between, to make a sentence.\n",
    "    Also create a dict for each token containing the token's text, start index, end index, and true label\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    A list of lists of lists; each list contains one sentence and its sublists are tokens, e.g.\n",
    "    [[['Melt', 'O'],\n",
    "     ['chocolate', 'B-FOOD'],\n",
    "     ['in', 'O'],\n",
    "     ['top', 'O'],\n",
    "     ['of', 'O'],\n",
    "     ['double', 'O'],\n",
    "     ['boiler', 'O']]]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dev_tokens: a list of lists of dicts; each list contains one sentence and its dicts are tokens, e.g.\n",
    "    dev_sentences: a list of strings, where each one is a rejoined sentence\n",
    "    '''\n",
    "    dev_tokens = []\n",
    "    dev_sentences = []\n",
    "\n",
    "    for example in dataset:\n",
    "        example_text = \" \".join([item[0] for item in example])\n",
    "        start = 0\n",
    "        example_tokens = []\n",
    "        for token in example:\n",
    "            text = token[0]\n",
    "            end = start + len(text)\n",
    "            assert text == example_text[start:end]\n",
    "            example_tokens.append({\"text\" : token[0], \"true_label\": token[1], \"start\" : start, \"end\": end})\n",
    "            start = end + 1\n",
    "        dev_tokens.append(example_tokens)\n",
    "        dev_sentences.append(example_text)\n",
    "    return dev_tokens, dev_sentences\n",
    "\n",
    "\n",
    "def add_predictions_to_tokens(predictions, token_list):\n",
    "    '''Iterate over the predicted entities and the original tokens, assigning predicted labels to the tokens\n",
    "      For each original token, this uses the predicted label for the first subtoken.\n",
    "     (processes a single sentence)\n",
    "     Parameters\n",
    "     ----------\n",
    "     predictions: a list of dicts produced by HuggingFace NER pipeline, e.g.\n",
    "     [{'entity': 'B-FOOD',\n",
    "      'score': 0.9986027,\n",
    "      'index': 13,\n",
    "      'word': 'salsa',\n",
    "      'start': 63,\n",
    "      'end': 68},\n",
    "     {'entity': 'B-FOOD',\n",
    "      'score': 0.9981828,\n",
    "      'index': 17,\n",
    "      'word': 'salt',\n",
    "      'start': 83,\n",
    "      'end': 87}]\n",
    "     token_list: a list of dicts produced by detokenize_conll_input\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    no explicit return; token_list is changed in place, with predictions added to the existing dicts\n",
    "    '''\n",
    "    result_start = 0 # for efficiency, can skip some preds once they've been mapped onto a token\n",
    "    for token in token_list:\n",
    "        for result in predictions[result_start:]:\n",
    "            if (result['start'] == token['start']):\n",
    "                token['pred_label'] = result['entity']\n",
    "                result_start += 1\n",
    "                break\n",
    "            token['pred_label'] = 'O'\n",
    "        if not \"pred_label\" in token:\n",
    "            token['pred_label'] = 'O'\n",
    "\n",
    "\n",
    "def split_label(label):\n",
    "    \"\"\"Split a label into the BIO tag and entity type.\"\"\"\n",
    "    if label.startswith(\"B-\"):\n",
    "        return \"B-\", label[2:]\n",
    "    elif label.startswith(\"I-\"):\n",
    "        return \"I-\", label[2:]\n",
    "    elif label.startswith(\"O\"):\n",
    "        return \"O\", \"O\"\n",
    "    else:\n",
    "        raise Exception(\"Found non-BIO label: {}!\".format(label))\n",
    "\n",
    "\n",
    "def correct_BIO_encodings(labels):\n",
    "    corrected_labels = []\n",
    "    curr_tag = \"O\"\n",
    "    for i, label in enumerate(labels):\n",
    "        BIO_tag, base_label = split_label(label)\n",
    "        if BIO_tag == \"B-\":\n",
    "            curr_tag = base_label\n",
    "            corrected_labels.append(label)\n",
    "        elif BIO_tag == \"I-\":\n",
    "            if base_label == curr_tag:\n",
    "                corrected_labels.append(label)\n",
    "            else:\n",
    "                corrected_labels.append(\"B-\" + base_label)\n",
    "                curr_tag = base_label\n",
    "        elif BIO_tag == \"O\":\n",
    "            corrected_labels.append(label)\n",
    "            curr_tag = \"O\"\n",
    "    return corrected_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1688615606575,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "QbS4T3IMGmnH"
   },
   "outputs": [],
   "source": [
    "dev_file = os.path.join(BASE_PATH, \"nlp_data/recipe_data/20200523_food_gold_dev.conll\")\n",
    "dev_dataset = read_conll_file(dev_file)\n",
    "dev_tokens, dev_sentences = detokenize_conll_input(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 3748,
     "status": "ok",
     "timestamp": 1688615612306,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "3iA_zqN9GmnH"
   },
   "outputs": [],
   "source": [
    "model_ckpt = os.path.join(BASE_PATH, 'food_ner_models/20230705_03_08_29-roberta-base-finetuned-ner/checkpoint-740')\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_ckpt)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1688615616816,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "CtcvuzwFGmnI"
   },
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 62655,
     "status": "ok",
     "timestamp": 1688615681821,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "xAy4Q6sgGmnI"
   },
   "outputs": [],
   "source": [
    "ner_results = nlp(dev_sentences, aggregation_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1688615685127,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "bNiJPmV6GmnI"
   },
   "outputs": [],
   "source": [
    "for result, token_list in zip(ner_results, dev_tokens):\n",
    "    add_predictions_to_tokens(result, token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1688616406267,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "z0M7hw5VGmnI"
   },
   "outputs": [],
   "source": [
    "# extract the labels and preds into a single list each for token-level metrics\n",
    "y_true_flat = []\n",
    "y_pred_flat = []\n",
    "\n",
    "# also make nested lists for writing out conll file\n",
    "y_true_nested = []\n",
    "y_pred_nested = []\n",
    "\n",
    "for sent in dev_tokens:\n",
    "    true_labels = [token['true_label'] for token in sent]\n",
    "    pred_labels = [token['pred_label'] for token in sent]\n",
    "    pred_labels = correct_BIO_encodings(pred_labels)\n",
    "    y_pred_flat.extend(pred_labels)\n",
    "    y_true_flat.extend(true_labels)\n",
    "    y_pred_nested.append(pred_labels)\n",
    "    y_true_nested.append(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 780,
     "status": "ok",
     "timestamp": 1688617365946,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "8gjzhhwKLOYi"
   },
   "outputs": [],
   "source": [
    "metrics = classification_report(y_true_flat, y_pred_flat, output_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1688617368810,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "unp95FD_Ow_u",
    "outputId": "0033bd33-b58e-4ae0-d92a-6b5404fd8d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-FOOD       0.96      0.97      0.97      1175\n",
      "      I-FOOD       0.95      0.94      0.94       278\n",
      "           O       1.00      0.99      1.00      9464\n",
      "\n",
      "    accuracy                           0.99     10917\n",
      "   macro avg       0.97      0.97      0.97     10917\n",
      "weighted avg       0.99      0.99      0.99     10917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1688617377630,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "1lI8K61WKesk"
   },
   "outputs": [],
   "source": [
    "# write out metrics\n",
    "outfile = os.path.join(BASE_PATH, \"nlp_data\", \"results\", \"20230705_03_08_29-roberta-base-finetuned-ner_dev_token_metrics.txt\")\n",
    "with open(outfile, \"w\") as out:\n",
    "  out.write(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1688616713840,
     "user": {
      "displayName": "Carol Anderson",
      "userId": "16112590610435518102"
     },
     "user_tz": 360
    },
    "id": "bpiibbpgGmnJ"
   },
   "outputs": [],
   "source": [
    "# write to conll format for use of conll perl script to calculate entity-level metrics\n",
    "outfile = os.path.join(BASE_PATH, \"nlp_data\", \"results\", \"20230705_03_08_29-roberta-base-finetuned-ner_dev_conll.txt\")\n",
    "with open(outfile, \"w\") as out:\n",
    "    ctr = 0\n",
    "    for doc, true, preds in zip(dev_tokens, y_true_nested, y_pred_nested):\n",
    "      for token, true_label, pred_label in zip(doc, true, preds):\n",
    "          assert token['true_label']  == true_label\n",
    "          out.write(f\"{token['text']} {token['true_label']} {pred_label}\\n\")  # use predictions with corrected BIO encodings\n",
    "      out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"/Users/carolanderson/Dropbox/nlp_data/results/20230705_03_08_29-roberta-base-finetuned-ner_dev_conll.txt\"\n",
    "outfile = '/Users/carolanderson/Dropbox/nlp_data/results/20230705_03_08_29-roberta-base-eval.txt'\n",
    "conlleval_script_path = \"/Users/carolanderson/Dropbox/repos/food/src/evaluation\"\n",
    "os.chdir(conlleval_script_path)\n",
    "cmd = \"perl conlleval.pl < {} > {}\".format(json.dumps(infile), json.dumps(outfile))\n",
    "os.system(cmd)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
