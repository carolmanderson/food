{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export annotations from Prodigy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from src.training.dataset_utils import train_dev_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Export data on command line:\n",
    "\n",
    "prodigy db-out food_ner_gold /Users/Carol/Documents/epicurious-recipes-with-rating-and-nutrition/labeled_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = \"/Users/Carol/Documents/epicurious-recipes-with-rating-and-nutrition/labeled_data/food_ner_gold_2.jsonl\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exported database is newline-delimited json.\n",
    "Sample line:\n",
    "{\"text\":\"Combine all ingredients in large bowl; toss to blend. Season salsa to taste with salt and pepper. Cover salsa and refrigerate at least 2 hours and up to 1 day, tossing occasionally.\",\"meta\":{\"row\":17006,\"subpart\":1,\"score\":0.5,\"pattern\":1},\"_input_hash\":-1530680985,\"_task_hash\":693554954,\"spans\":[{\"text\":\"salt\",\"start\":81,\"end\":85,\"priority\":0.5,\"score\":0.5,\"pattern\":-686066741,\"label\":\"FOOD\"}],\"_session_id\":\"food_ner-default\",\"_view_id\":\"ner\",\"answer\":\"accept\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count records and see if they're complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = defaultdict(list)\n",
    "with open(db_file, 'r') as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(line)\n",
    "        num = record['meta']['row']\n",
    "        part = record['meta']['subpart']\n",
    "        records[num].append(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to input data\n",
    "orig_file = \"/Users/Carol/Documents/epicurious-recipes-with-rating-and-nutrition/use_directions.jsonl\"\n",
    "orig_records = defaultdict(list)\n",
    "with open(db_file, 'r') as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(line)\n",
    "        num = record['meta']['row']\n",
    "        part = record['meta']['subpart']\n",
    "        orig_records[num].append(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rec in records:\n",
    "    assert orig_records[rec] == records[rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n",
      "981\n"
     ]
    }
   ],
   "source": [
    "# count labeled records\n",
    "recipes = 0\n",
    "sentences = 0\n",
    "for rec in records:\n",
    "    recipes += 1\n",
    "    sentences += len(records[rec])\n",
    "print(recipes)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to CoNLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_overlap(token_1, token_2):\n",
    "    token_1_range = {i for i in range(token_1['start'], token_1['end'])}\n",
    "    token_2_range = {i for i in range(token_2['start'], token_2['end'])}\n",
    "    if token_1_range.intersection(token_2_range):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def assign_labels_to_tokens(token_spans, label_spans):\n",
    "    token_spans = copy.deepcopy(token_spans)\n",
    "    for tok in token_spans:\n",
    "        for lab in label_spans:\n",
    "            if tokens_overlap(tok, lab):\n",
    "                tok['label'] = lab['label']\n",
    "    return token_spans\n",
    "\n",
    "def assign_metadata_to_tokens(token_spans, record_id):\n",
    "    token_spans = copy.deepcopy(token_spans)\n",
    "    for token in token_spans:\n",
    "        token['record_id'] = record_id\n",
    "    return token_spans\n",
    "\n",
    "def add_bio_tags(token_spans):\n",
    "    token_spans = copy.deepcopy(token_spans)\n",
    "    prev_label = 'O'\n",
    "    for tok in token_spans:\n",
    "        label = tok.get('label', 'O')\n",
    "        if label == 'O':\n",
    "            tok['bio-tag'] = 'O'\n",
    "            prev_label = 'O'\n",
    "            continue\n",
    "        elif label == prev_label:\n",
    "            tok['bio-tag'] = \"I-\" + label\n",
    "            prev_label = label\n",
    "        else:\n",
    "            tok['bio-tag'] = \"B-\" + label\n",
    "            prev_label = label\n",
    "    return token_spans\n",
    "\n",
    "\n",
    "def get_token_and_label_spans_from_jsonl(db_file):\n",
    "    \"\"\"Returns a nested list where each document is a sublist.\"\"\"\n",
    "    token_spans = []\n",
    "    label_spans = []\n",
    "    record_ids = []\n",
    "    with open(db_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            token_spans.append(record['tokens'])\n",
    "            label_spans.append(record['spans'])\n",
    "            num = record['meta']['row']\n",
    "            part = record['meta']['subpart']\n",
    "            record_ids.append(str(num) + \"_\" + str(part))\n",
    "    return token_spans, label_spans, record_ids\n",
    "\n",
    "\n",
    "def get_bio_tagged_spans_from_jsonl(db_file):\n",
    "    result = []\n",
    "    token_spans, label_spans, record_ids = get_token_and_label_spans_from_jsonl(db_file)\n",
    "    for tok, lab, rec_id in zip(token_spans, label_spans, record_ids):\n",
    "        tagged_spans = assign_labels_to_tokens(tok, lab)\n",
    "        tagged_spans = assign_metadata_to_tokens(tagged_spans, rec_id)\n",
    "        bio_tagged_spans = add_bio_tags(tagged_spans)\n",
    "        result.append(bio_tagged_spans)\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_conll(bio_tagged_spans, outfile):\n",
    "    with open(outfile, 'w') as out:\n",
    "        for sent in bio_tagged_spans:\n",
    "            for tok in sent:\n",
    "                out.write(f\"{tok['text']} {tok['start']} {int(tok['end'])+len(tok['text'])} {tok['record_id']} {tok['bio-tag']}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "                \n",
    "token_spans = get_bio_tagged_spans_from_jsonl(db_file)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "981"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training, dev, and test sets\n",
    "train, dev, test = train_dev_test_split(token_spans, 0.6, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588 196 197\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(dev), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfilestem = \"/Users/Carol/Documents/epicurious-recipes-with-rating-and-nutrition/food_gold_{}.conll\"\n",
    "for dataset, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n",
    "    write_conll(dataset, outfilestem.format(name))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To import back to Prodigy\n",
    "\n",
    "prodigy db-in [dataset] [in_file] [--loader] [--answer] [--overwrite] [--dry]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prodigy] *",
   "language": "python",
   "name": "conda-env-prodigy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
