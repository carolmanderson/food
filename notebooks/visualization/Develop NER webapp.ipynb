{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from food_tools.training.dataset_utils import tokens_to_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_displacy(tokens, labels):\n",
    "    text = \"\"\n",
    "    start = 0\n",
    "    ents = []\n",
    "    curr_label = \"\"\n",
    "    new_ent = {}\n",
    "    for token, label in zip(tokens, labels):\n",
    "        text += token + \" \"\n",
    "        end = start + len(token)\n",
    "        if label.startswith(\"B-\"):\n",
    "            if new_ent:\n",
    "                ents.append(new_ent)\n",
    "            curr_label = label[2:]\n",
    "            new_ent = {\"start\": start, \"end\": end,\n",
    "                       \"label\": curr_label}\n",
    "        elif label.startswith(\"I-\"):\n",
    "            assert label[2:] == curr_label\n",
    "            new_ent['end'] = end\n",
    "        elif label == \"O\":\n",
    "            if new_ent:\n",
    "                ents.append(new_ent)\n",
    "                new_ent = {}\n",
    "        else:\n",
    "            raise Exception(\"Found non-BIO label {}!\".format(label))\n",
    "        start += len(token) + 1\n",
    "    if new_ent:\n",
    "        ents.append(new_ent)\n",
    "    doc = {\"text\": text,\n",
    "           \"ents\": ents,\n",
    "           \"title\": None}\n",
    "    return doc\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    https://github.com/tensorflow/tensorflow/issues/14356\n",
    "    https://github.com/tensorflow/tensorflow/issues/28287\n",
    "    \"\"\"\n",
    "    session = tf.Session(graph=tf.Graph())\n",
    "    with session.graph.as_default():\n",
    "        K.set_session(session)\n",
    "        loaded_model = tf.keras.models.load_model(model_path)\n",
    "        loaded_model.summary()\n",
    "    return loaded_model, session\n",
    "\n",
    "\n",
    "def load_mappings(filepath):\n",
    "    return pickle.load(open(filepath, \"rb\"))\n",
    "\n",
    "\n",
    "def load_sentencizer_and_tokenizer():\n",
    "    nlp = English()\n",
    "    sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "    nlp.add_pipe(sentencizer)\n",
    "    tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "    return nlp, tokenizer\n",
    "\n",
    "\n",
    "def form_matrix(tokens):\n",
    "    tokens = np.expand_dims(tokens, axis=0)\n",
    "    return np.array(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = \"/Users/Carol/Google Drive/nlp_data/output/20200503_16_50_50/20200503_16_50_50_food_ner_epoch_3_dev_f1_0.9867637173043644.h5\"\n",
    "\n",
    "saved_mappings = \"/Users/Carol/Google Drive/nlp_data/output/20200503_16_50_50/20200503_16_50_50_food_ner_mappings.pkl\"\n",
    "\n",
    "model, session = load_model(saved_model)\n",
    "mappings = load_mappings(saved_mappings)\n",
    "index_to_label = {v: k for k, v in mappings['label_to_index'].items()}\n",
    "token_to_index = mappings['token_to_index']\n",
    "sentencizer, tokenizer  = load_sentencizer_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"Heat garlic and rosemary with oil. Drizzle oil over dip and serve with \" \\\n",
    "                    \"vegetables.\"\n",
    "text = \"Combine pineapple, banana, cream of coconut, rolled \" \\\n",
    "                \"oats, quick-cooking oats, baking powder, mint, \" \\\n",
    "                \"chia seeds, and poppy seeds in a blender; blend until \" \\\n",
    "                \"smooth. Pour into 2 mugs.\"\n",
    "\n",
    "\n",
    "\n",
    "sents = sentencizer(text)\n",
    "all_tokens = []\n",
    "for sent in sents.sents:\n",
    "    tokens = tokenizer(sent.text)\n",
    "    all_tokens.append([t.text for t in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc = None   # collects results from all sentences\n",
    "with session.graph.as_default():\n",
    "    K.set_session(session)\n",
    "    for tokens in all_tokens:\n",
    "        token_indices = tokens_to_indices(tokens, token_to_index)\n",
    "        preds = model.predict([tokens_to_indices(tokens, token_to_index)])\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "        labels = [index_to_label[ind[0]] for ind in preds]\n",
    "        labels = correct_BIO_encodings(labels)\n",
    "        doc = output_to_displacy(tokens, labels)\n",
    "        if not final_doc:  # first sentence\n",
    "            final_doc = doc\n",
    "            continue\n",
    "        shift = len(final_doc['text'])\n",
    "        for ent in doc['ents']:\n",
    "            ent['start'] += shift\n",
    "            ent['end'] += shift\n",
    "            final_doc['ents'].append(ent)\n",
    "        final_doc['text'] += doc['text']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"FOOD\": \"#87CEEB\"}\n",
    "options = {\"ents\": [\"FOOD\"], \"colors\": colors}\n",
    "displacy.render(final_doc, style=\"ent\", options={\"colors\":colors},\n",
    "                       manual=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get examples from dev set\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "from food_tools.training.dataset_utils import read_conll_file, compile_vocabulary, make_label_map, get_token_embeddings, examples_to_indices, tokens_to_indices\n",
    "from food_tools.training.train_utils import get_current_time, form_ner_train_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/Carol/Google Drive/\"\n",
    "dev_file = os.path.join(base_path, \"nlp_data/recipe_data/20200523_food_gold_test.conll\")\n",
    "dev_dataset = read_conll_file(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = random.sample(dev_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in examples:\n",
    "    text = \"\"\n",
    "    for token in example:\n",
    "\n",
    "        text += (token[0] + \" \")\n",
    "        \n",
    "    print(text)\n",
    "    print(\"========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update 2023, make a couple of examples for new gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"FOOD\": \"#87CEEB\"}\n",
    "options = {\"ents\": [\"FOOD\"], \"colors\": colors}\n",
    "\n",
    "doc = {\"text\": \"Salt the water and butter the bread\",\n",
    "       \"ents\": [{\"start\" : 9, \"end\": 14, \"label\" : \"FOOD\"},\n",
    "               {\"start\" : 30, \"end\": 35, \"label\" : \"FOOD\"}],\n",
    "       \"title\": None}\n",
    "svg = displacy.render(doc, style=\"ent\", options={\"colors\":colors},\n",
    "                       manual=True)\n",
    "\n",
    "# output_path = Path(\"/Users/carolanderson/Dropbox/repos/food_gradio_app/app_images/salt_butter_new.svg\")\n",
    "# output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = {\"text\": \"Salt the water and butter the bread\",\n",
    "       \"ents\": [{\"start\" : 0, \"end\": 4, \"label\" : \"FOOD\"},\n",
    "               {\"start\" : 9, \"end\": 14, \"label\" : \"FOOD\"},\n",
    "                {\"start\": 19, \"end\": 25, \"label\": \"FOOD\"},\n",
    "               {\"start\" : 30, \"end\": 35, \"label\" : \"FOOD\"}],\n",
    "       \"title\": None}\n",
    "displacy.render(doc, style=\"ent\", options={\"colors\":colors},\n",
    "                       manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf_space]",
   "language": "python",
   "name": "conda-env-hf_space-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
